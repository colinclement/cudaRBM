{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import numpy\n",
      "import os\n",
      "from numpy import exp, sqrt, zeros, dot, log, sum, mean, round\n",
      "from Saverz_and_Loaderz import *\n",
      "from scipy.linalg import sqrtm, inv\n",
      "\n",
      "\n",
      "class RBM(object):\n",
      "    \"\"\"Restricted Boltzmann Machine (RBM)  \"\"\"\n",
      "    def __init__(self, input=None, n_visible=256, n_hidden=30, W=None, hbias=None, vbias=None, numpy_rng=None):\n",
      "        \n",
      "        self.n_visible = n_visible\n",
      "        self.n_hidden = n_hidden\n",
      "        self.bit = 0\n",
      "        \n",
      "        if numpy_rng is None:\n",
      "            self.numpy_rng = numpy.random.RandomState(1234)\n",
      "            \n",
      "        #Need to initialize these to a reasonable value\n",
      "        if W is None:\n",
      "            W = numpy.asarray(self.numpy_rng.uniform(\n",
      "                    low=-4 * sqrt(6. / (n_hidden + n_visible)),\n",
      "                    high=4 * sqrt(6. / (n_hidden + n_visible)),\n",
      "                    size=(n_visible, n_hidden)))\n",
      "            W = self.sym(W)\n",
      "\n",
      "        if hbias is None:\n",
      "            hbias = zeros(n_hidden)\n",
      "            \n",
      "        if vbias is None:\n",
      "            vbias = zeros(n_visible)\n",
      "            \n",
      "        self.input = input\n",
      "        self.W = W\n",
      "        self.hbias = hbias\n",
      "        self.vbias = vbias\n",
      "        self.params = [self.W, self.hbias, self.vbias]\n",
      "        pre_sigmoid_ph, ph_mean, ph_sample = self.sample_h_given_v(self.input)\n",
      "        self.persistent_value = ph_sample\n",
      "    \n",
      "    def sym(self, w):\n",
      "        return w.dot(inv(sqrtm(w.T.dot(w))))   \n",
      "    \n",
      "    def sigmoid(self,x):\n",
      "        \"\"\"sigmoid function\"\"\"\n",
      "        y = 1/(1+exp(-x))\n",
      "        return y\n",
      "\n",
      "    def free_energy(self, v_sample):\n",
      "        ''' Function to compute the free energy '''\n",
      "        wx_b = dot(v_sample, self.W) + self.hbias\n",
      "        vbias_term = dot(v_sample, self.vbias)\n",
      "        hidden_term = sum(log(1 + exp(wx_b)), axis=1)\n",
      "        return -hidden_term - vbias_term\n",
      "\n",
      "    def propup(self, vis):\n",
      "        '''This function propagates the visible units activation upwards to\n",
      "        the hidden units\n",
      "        '''\n",
      "        pre_sigmoid_activation = vis.dot(self.W) + self.hbias\n",
      "        return [pre_sigmoid_activation, self.sigmoid(pre_sigmoid_activation)]\n",
      "\n",
      "    def sample_h_given_v(self, v0_sample):\n",
      "        ''' This function infers state of hidden units given visible units '''\n",
      "        pre_sigmoid_h1, h1_mean = self.propup(v0_sample)\n",
      "        h1_sample = self.numpy_rng.binomial(size=h1_mean.shape,n=1, p=h1_mean)\n",
      "        return [pre_sigmoid_h1, h1_mean, h1_sample]\n",
      "\n",
      "    def propdown(self, hid):\n",
      "        '''This function propagates the hidden units activation downwards to\n",
      "        the visible units\n",
      "        '''\n",
      "        pre_sigmoid_activation = hid.dot(self.W.T) + self.vbias\n",
      "        return [pre_sigmoid_activation, self.sigmoid(pre_sigmoid_activation)]\n",
      "\n",
      "    def sample_v_given_h(self, h0_sample):\n",
      "        ''' This function infers state of visible units given hidden units '''\n",
      "        pre_sigmoid_v1, v1_mean = self.propdown(h0_sample)\n",
      "        v1_sample = self.numpy_rng.binomial(size=v1_mean.shape, n=1, p=v1_mean)\n",
      "        return [pre_sigmoid_v1, v1_mean, v1_sample]\n",
      "\n",
      "    def gibbs_hvh(self, h0_sample):\n",
      "        ''' This function implements one step of Gibbs sampling,\n",
      "            starting from the hidden state'''\n",
      "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
      "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n",
      "        return [pre_sigmoid_v1, v1_mean, v1_sample,\n",
      "                pre_sigmoid_h1, h1_mean, h1_sample]\n",
      "\n",
      "    def gibbs_vhv(self, v0_sample):\n",
      "        ''' This function implements one step of Gibbs sampling,\n",
      "            starting from the visible state'''\n",
      "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\n",
      "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\n",
      "        return [pre_sigmoid_h1, h1_mean, h1_sample,\n",
      "                pre_sigmoid_v1, v1_mean, v1_sample]\n",
      "    \n",
      "    def get_cost_updates(self, n_visible=256, n_hidden=30, lr=0.1, persistent=False, k=1):\n",
      "        \"\"\"This functions implements one step of CD-k or PCD-k\n",
      "        :param lr: learning rate used to train the RBM\n",
      "        :param persistent: None for CD. For PCD, variable\n",
      "            containing old state of Gibbs chain. This must be a shared\n",
      "            variable of size (batch size, number of hidden units).\n",
      "        :param k: number of Gibbs steps to do in CD-k/PCD-k\n",
      "        \"\"\"\n",
      "        pre_sigmoid_ph, ph_mean, ph_sample = self.sample_h_given_v(self.input)\n",
      "        \n",
      "        if persistent:\n",
      "            chain_start = self.persistent_value\n",
      "        else:\n",
      "            chain_start = ph_sample\n",
      "        \n",
      "        n_samples = self.input.shape[0]\n",
      "        pre_sigmoid_nvs = zeros((k, n_samples, n_visible))\n",
      "        nv_means = zeros((k, n_samples, n_visible))\n",
      "        nv_samples = zeros((k, n_samples, n_visible))\n",
      "        pre_sigmoid_nhs = zeros((k, n_samples, n_hidden))\n",
      "        nh_means = zeros((k, n_samples, n_hidden))\n",
      "        nh_samples =zeros((k, n_samples, n_hidden))\n",
      "        for i in xrange(k):\n",
      "            if i == 0:\n",
      "                pre_sigmoid_nvs[i],nv_means[i],nv_samples[i],pre_sigmoid_nhs[i],nh_means[i],nh_samples[i] = self.gibbs_hvh(chain_start)\n",
      "            else:\n",
      "                pre_sigmoid_nvs[i],nv_means[i],nv_samples[i],pre_sigmoid_nhs[i],nh_means[i],nh_samples[i] = self.gibbs_hvh(nh_samples[k-1])\n",
      "        chain_end = nv_samples[-1]\n",
      "     \n",
      "        cost = mean(self.free_energy(self.input)) - mean(self.free_energy(chain_end))\n",
      "        gparams = self.grad(chain_end)\n",
      "        self.W += lr*gparams[0]\n",
      "        self.hbias += lr*gparams[1]\n",
      "        self.vbias += lr*gparams[2]\n",
      "        self.params = [self.W, self.hbias, self.vbias]\n",
      "\n",
      "        if persistent:\n",
      "            self.persistent_value = nh_samples[-1]\n",
      "            monitoring_cost = self.get_pseudo_likelihood_cost()\n",
      "        else:\n",
      "            monitoring_cost = self.get_reconstruction_cost(pre_sigmoid_nvs[-1])\n",
      "\n",
      "        return monitoring_cost\n",
      "\n",
      "    def grad(self, chain_end):\n",
      "        \"\"\" Calculate the gradient of the cost wrt to parameters\"\"\"\n",
      "        g_W = mean(self.useful_func(chain_end), axis=0) - mean(self.useful_func(self.input), axis=0)\n",
      "        g_hbias = mean(self.useful_func2(chain_end), axis=0) - mean(self.useful_func2(self.input), axis=0)\n",
      "        g_vbias = mean(chain_end, axis=0) - mean(self.input, axis=0)\n",
      "        return [g_W, g_hbias, g_vbias]\n",
      "    \n",
      "\n",
      "    def useful_func(self,x):\n",
      "        nv = self.n_visible\n",
      "        nh = self.n_hidden\n",
      "        piece = self.sigmoid(x.dot(self.W)+self.hbias)# n_samples x n_hidden\n",
      "        part = numpy.einsum('il,im->ilm', x, piece)\n",
      "        return part\n",
      "    \n",
      "  \n",
      "    def useful_func2(self, vis):\n",
      "        return self.sigmoid(dot(vis, self.W) + self.hbias)\n",
      "    \n",
      "      \n",
      "    def get_pseudo_likelihood_cost(self):\n",
      "        \"\"\"Stochastic approximation to the pseudo-likelihood\"\"\"\n",
      "        bit_i_idx = self.bit\n",
      "        xi = round(self.input)\n",
      "        fe_xi = self.free_energy(xi)\n",
      "        xi_flip = xi.copy()\n",
      "        xi_flip[:,bit_i_idx] =  1 - xi[:, bit_i_idx]\n",
      "        fe_xi_flip = self.free_energy(xi_flip)\n",
      "        cost = numpy.mean(self.n_visible * numpy.log(self.sigmoid(fe_xi_flip - fe_xi)))\n",
      "        self.bit = (self.bit + 1) % self.n_visible\n",
      "        return cost\n",
      "\n",
      "    def get_reconstruction_cost(self,pre_sigmoid_nv):\n",
      "        \"\"\"Approximation to the reconstruction error\"\"\"\n",
      "        cross_entropy = mean(sum(self.input*log(self.sigmoid(pre_sigmoid_nv))+(1 - self.input)*log(1-self.sigmoid(pre_sigmoid_nv)), axis=1))\n",
      "        return cross_entropy\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}